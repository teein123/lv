import streamlit as st
import akshare as ak
import pandas as pd
import numpy as np
import datetime
import traceback
import time
import warnings
import requests
import random
from requests.sessions import Session

# ==========================================
# 0. é¡µé¢é…ç½®ä¸è¡¥ä¸ (æ”¾åœ¨æœ€å‰é¢)
# ==========================================
st.set_page_config(
    page_title="AIç¼ è®ºæŠ•å–‚ç³»ç»Ÿ v6.3",
    page_icon="ğŸ“ˆ",
    layout="wide"
)

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings("ignore")

# --- ç½‘ç»œè¯·æ±‚è¡¥ä¸ (ä¿æŒåŸç‰ˆé€»è¾‘) ---
_original_request = Session.request

def patched_request(self, method, url, *args, **kwargs):
    # 1. å¼ºåˆ¶æ¸…ç©ºä»£ç†
    kwargs['proxies'] = {"http": None, "https": None}
    # 2. å¿½ç•¥ SSL
    kwargs['verify'] = False
    # 3. å¼ºåˆ¶æ·»åŠ ä¼ªè£…å¤´
    if 'headers' not in kwargs:
        kwargs['headers'] = {}
    if not kwargs['headers'].get('User-Agent'):
        kwargs['headers']['User-Agent'] = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        kwargs['headers']['Referer'] = 'http://quote.eastmoney.com/'
        
    try:
        return _original_request(self, method, url, *args, **kwargs)
    except Exception as e:
        raise e

# åº”ç”¨è¡¥ä¸
Session.request = patched_request

# ==========================================
# 1. æ ¸å¿ƒç®—æ³• (é€»è¾‘ä¿æŒä¸å˜)
# ==========================================
def calculate_macd(df):
    df = df.copy()
    close = df['close'].values
    
    def calc_ema_recursive(series, span):
        alpha = 2 / (span + 1)
        ema = np.zeros_like(series)
        ema[0] = series[0] 
        for i in range(1, len(series)):
            ema[i] = alpha * series[i] + (1 - alpha) * ema[i-1]
        return ema

    df['ema12'] = calc_ema_recursive(close, 12)
    df['ema26'] = calc_ema_recursive(close, 26)
    df['dif'] = df['ema12'] - df['ema26']
    df['dea'] = calc_ema_recursive(df['dif'].values, 9)
    df['macd'] = (df['dif'] - df['dea']) * 2
    return df

def get_segment_metrics_by_date(raw_df, start_date, end_date, direction):
    mask = (raw_df['date'] >= start_date) & (raw_df['date'] <= end_date)
    segment_df = raw_df.loc[mask].copy()
    if segment_df.empty: return 0.0, 0.0, 0.0

    if direction == 'å‘ä¸Š':
        macd_area = segment_df[segment_df['macd'] > 0]['macd'].sum()
        try:
            idx_price = segment_df['high'].idxmax()
            peak_dif = segment_df.loc[idx_price, 'dif']
        except:
            peak_dif = 0
    else:
        macd_area = abs(segment_df[segment_df['macd'] < 0]['macd'].sum())
        try:
            idx_price = segment_df['low'].idxmin()
            peak_dif = segment_df.loc[idx_price, 'dif']
        except:
            peak_dif = 0
    
    avg_vol = segment_df['volume'].mean() / 10000 if not segment_df.empty else 0
    
    return round(macd_area, 4), round(peak_dif, 4), round(avg_vol, 2)

def preprocess_inclusion(df):
    if len(df) < 2: return df
    raw_data = df.to_dict('records')
    for d in raw_data: 
        if 'real_date' not in d: d['real_date'] = d['date']

    processed = [raw_data[0]]
    direction = 1 
    if raw_data[1]['close'] < raw_data[0]['close']: direction = -1

    for i in range(1, len(raw_data)):
        cur = raw_data[i]
        last = processed[-1]
        
        is_cur_inside = (cur['high'] <= last['high'] and cur['low'] >= last['low'])
        is_last_inside = (cur['high'] >= last['high'] and cur['low'] <= last['low'])
        
        if is_cur_inside or is_last_inside:
            last['volume'] = float(last['volume']) + float(cur['volume'])
            last['date'] = cur['date']
            last['close'] = cur['close']
            
            last_amp = (last['high'] - last['low']) / last['low'] if last['low'] > 0 else 0
            if last_amp > 0.015: 
                last['high'] = max(last['high'], cur['high'])
                last['low'] = min(last['low'], cur['low']) 
            else:
                if direction == 1:
                    last['high'] = max(last['high'], cur['high'])
                    last['low'] = max(last['low'], cur['low'])
                else:
                    last['high'] = min(last['high'], cur['high'])
                    last['low'] = min(last['low'], cur['low'])
            
            if direction == 1 and cur['high'] == last['high']: last['real_date'] = cur['real_date']
            elif direction == -1 and cur['low'] == last['low']: last['real_date'] = cur['real_date']
        else:
            if cur['high'] > last['high'] and cur['low'] > last['low']: direction = 1
            elif cur['high'] < last['high'] and cur['low'] < last['low']: direction = -1
            processed.append(cur)
            
    return pd.DataFrame(processed)

def calculate_chanlun_structure(df):
    if len(df) < 10: return [] 

    k_df = preprocess_inclusion(df)
    k_df = k_df.reset_index(drop=True)
    if len(k_df) < 5: return [] 

    k_df['type'] = 0 
    for i in range(1, len(k_df)-1):
        prev, curr, next_ = k_df.iloc[i-1], k_df.iloc[i], k_df.iloc[i+1]
        if curr['high'] > prev['high'] and curr['high'] > next_['high'] and curr['low'] > prev['low'] and curr['low'] > next_['low']:
            k_df.loc[i, 'type'] = 1 
        elif curr['low'] < prev['low'] and curr['low'] < next_['low'] and curr['high'] < prev['high'] and curr['high'] < next_['high']:
            k_df.loc[i, 'type'] = -1 
        
        if k_df.loc[i, 'type'] == 0:
            if curr['high'] >= prev['high'] and curr['high'] >= next_['high'] and curr['high'] > max(prev['high'], next_['high']):
                 k_df.loc[i, 'type'] = 1
            elif curr['low'] <= prev['low'] and curr['low'] <= next_['low'] and curr['low'] < min(prev['low'], next_['low']):
                 k_df.loc[i, 'type'] = -1

    fractals = k_df[k_df['type'] != 0].copy()
    if len(fractals) < 2: return []
    
    stack = [fractals.iloc[0]]
    for i in range(1, len(fractals)):
        curr = fractals.iloc[i]
        last = stack[-1]
        
        if curr['type'] == last['type']:
            if curr['type'] == 1 and curr['high'] > last['high']:
                stack.pop(); stack.append(curr)
            elif curr['type'] == -1 and curr['low'] < last['low']:
                stack.pop(); stack.append(curr)
            continue
            
        if curr.name - last.name >= 4:
            is_valid_space = False
            if last['type'] == 1 and curr['type'] == -1 and curr['low'] < last['low']: is_valid_space = True
            if last['type'] == -1 and curr['type'] == 1 and curr['high'] > last['high']: is_valid_space = True
            if (curr.name - last.name >= 9): is_valid_space = True
            if is_valid_space: stack.append(curr)
            
    bi_list = []
    if len(stack) < 2: return []
    
    for i in range(1, len(stack)):
        start_node = stack[i-1]
        end_node = stack[i]
        bi_dir = 'å‘ä¸Š' if start_node['type'] == -1 else 'å‘ä¸‹'
        a, p, v = get_segment_metrics_by_date(df, start_node['real_date'], end_node['real_date'], bi_dir)
        
        bi_list.append({
            'start_date': start_node['date'], 'end_date': end_node['date'],
            'display_start_date': start_node['real_date'], 'display_end_date': end_node['real_date'],
            'start_price': float(start_node['low']) if start_node['type'] == -1 else float(start_node['high']),
            'end_price': float(end_node['low']) if end_node['type'] == -1 else float(end_node['high']),
            'direction': bi_dir, 'macd_area': a, 'peak_dif': p, 'avg_vol': v
        })
            
    return bi_list

def analyze_unformed_segment(df, last_bi):
    last_end_date = last_bi['end_date']
    unformed_df = df[df['date'] > last_end_date].copy()
    if len(unformed_df) == 0: return None
    
    last_dir = last_bi['direction']
    last_end_price = last_bi['end_price']
    
    current_dir = 'å‘ä¸‹' if last_dir == 'å‘ä¸Š' else 'å‘ä¸Š'
    status_note = "ã€æ­£å¸¸å›è°ƒã€‘"
    
    if last_dir == 'å‘ä¸Š':
        if unformed_df['high'].max() > last_end_price:
            current_dir = 'å‘ä¸Š'; status_note = "ã€å¼ºåŠ¿å»¶ç»­ã€‘"
    else: 
        if unformed_df['low'].min() < last_end_price:
            current_dir = 'å‘ä¸‹'; status_note = "ã€ä¸‹è·Œä¸­ç»§ã€‘"

    physical_count = len(unformed_df)
    logical_df = preprocess_inclusion(unformed_df)
    logical_count = len(logical_df)
    current_avg_vol = unformed_df['volume'].mean() / 10000

    if current_dir == 'å‘ä¸Š':
        macd_area = unformed_df[unformed_df['macd'] > 0]['macd'].sum()
        try:
            peak_dif = unformed_df.loc[unformed_df['high'].idxmax(), 'dif']
        except: peak_dif = 0
    else:
        macd_area = abs(unformed_df[unformed_df['macd'] < 0]['macd'].sum())
        try:
            peak_dif = unformed_df.loc[unformed_df['low'].idxmin(), 'dif']
        except: peak_dif = 0

    return {
        'count': physical_count, 'logical_count': logical_count,
        'high': unformed_df['high'].max(), 'low': unformed_df['low'].min(), 'close': unformed_df.iloc[-1]['close'],
        'macd_area': round(macd_area, 4), 'peak_dif': round(peak_dif, 4),
        'avg_vol': round(current_avg_vol, 2), 
        'current_dir': current_dir,
        'status': status_note
    }

# ==========================================
# 2. æ•°æ®è·å– (ä½¿ç”¨ç¼“å­˜)
# ==========================================
@st.cache_data(ttl=300, show_spinner=False)
def get_stock_data(code, freq='d'):
    """ ä»ä¸œæ–¹è´¢å¯Œè·å–ä¼˜è´¨æ•°æ®ï¼Œä½¿ç”¨ Streamlit ç¼“å­˜é¿å…é‡å¤è¯·æ±‚ """
    # æ¨¡æ‹Ÿéšæœºå»¶è¿Ÿï¼Œè™½ç„¶ç¼“å­˜äº†ï¼Œä½†åˆæ¬¡è¯·æ±‚è¿˜æ˜¯æ¨¡æ‹Ÿä¸€ä¸‹å¥½
    sleep_time = random.uniform(0.5, 1.0)
    time.sleep(sleep_time)
    
    pure_code = code.split('.')[-1]
    
    try:
        start_date = (datetime.date.today() - datetime.timedelta(days=730)).strftime('%Y%m%d')
        end_date = datetime.date.today().strftime('%Y%m%d')

        if freq == 'd':
            df = ak.stock_zh_a_hist(symbol=pure_code, period="daily", start_date=start_date, end_date=end_date, adjust="qfq")
            if not df.empty:
                df = df.rename(columns={'æ—¥æœŸ':'date','å¼€ç›˜':'open','æœ€é«˜':'high','æœ€ä½':'low','æ”¶ç›˜':'close','æˆäº¤é‡':'volume'})
        elif freq == '30':
            df = ak.stock_zh_a_hist_min_em(symbol=pure_code, period='30', adjust='qfq')
            if not df.empty:
                df = df.rename(columns={'æ—¶é—´':'date','å¼€ç›˜':'open','æœ€é«˜':'high','æœ€ä½':'low','æ”¶ç›˜':'close','æˆäº¤é‡':'volume'})
        elif freq == '5':
            df = ak.stock_zh_a_hist_min_em(symbol=pure_code, period='5', adjust='qfq')
            if not df.empty:
                df = df.rename(columns={'æ—¶é—´':'date','å¼€ç›˜':'open','æœ€é«˜':'high','æœ€ä½':'low','æ”¶ç›˜':'close','æˆäº¤é‡':'volume'})
            
        if not df.empty:
            df['date'] = pd.to_datetime(df['date'])
            cols = ['open','high','low','close','volume']
            for c in cols: df[c] = pd.to_numeric(df[c])
            df = df.sort_values('date').reset_index(drop=True)
            return df
        else:
            return pd.DataFrame()
            
    except Exception as e:
        # st.error(f"{freq}çº¿è·å–å¤±è´¥: {e}")
        return pd.DataFrame()

# ==========================================
# 3. Streamlit ä¸»ç¨‹åº
# ==========================================
def main():
    st.title("AIç¼ è®ºæŠ•å–‚ç³»ç»Ÿ v6.3 (Webç‰ˆ)")
    st.markdown("### ç‰¹æ€§: å¼ºåˆ¶ç›´è¿å»ä»£ç† | æ—¥çº¿/30åˆ†/5åˆ† è”ç«‹")
    
    with st.sidebar:
        st.header("è®¾ç½®")
        stock_code = st.text_input("è¯·è¾“å…¥è‚¡ç¥¨ä»£ç ", value="600885", help="ä¾‹å¦‚ 600885, ä¸éœ€è¦åŠ åç¼€").strip()
        run_btn = st.button("å¼€å§‹åˆ†æ", type="primary")
        
        st.info("è¯´æ˜ï¼šç‚¹å‡»åˆ†æåï¼Œç³»ç»Ÿä¼šè·å–æ•°æ®å¹¶ç”Ÿæˆç¼ è®ºç»“æ„ Promptï¼Œå¯ç›´æ¥å¤åˆ¶ç»™ GPT/Claude ä½¿ç”¨ã€‚")

    if run_btn and stock_code:
        pure_code = stock_code.split('.')[-1]
        
        status_container = st.status(f"æ­£åœ¨åˆ†æ {pure_code}...", expanded=True)
        
        try:
            # --- 1. æ—¥çº¿å¤„ç† ---
            status_container.write("ğŸ“¥ æ­£åœ¨ä¸‹è½½æ—¥çº¿æ•°æ®...")
            df_d = get_stock_data(pure_code, 'd')
            
            if df_d.empty: 
                status_container.update(label="âŒ æ•°æ®è·å–å¤±è´¥", state="error")
                st.error("æ— æ³•è·å–æ—¥çº¿æ•°æ®ï¼Œè¯·æ£€æŸ¥ä»£ç æ˜¯å¦æ­£ç¡®æˆ–IPæ˜¯å¦è¢«é™åˆ¶ã€‚")
                return

            df_d = calculate_macd(df_d)
            
            # éªŒé’æœºå±•ç¤º
            st.subheader("ğŸ§ æ•°æ®éªŒé’æœº (æœ€è¿‘3æ—¥)")
            tail_df = df_d.tail(3)[['date', 'close', 'dif', 'dea', 'macd']].copy()
            tail_df['date'] = tail_df['date'].dt.strftime('%Y-%m-%d')
            st.dataframe(tail_df, hide_index=True)
            
            status_container.write("ğŸ§® è®¡ç®—æ—¥çº¿ç¼ è®ºç»“æ„...")
            bi_d = calculate_chanlun_structure(df_d)
            
            # --- 2. 30åˆ†é’Ÿå¤„ç† ---
            status_container.write("ğŸ“¥ æ­£åœ¨ä¸‹è½½30åˆ†é’Ÿæ•°æ®...")
            df_30 = get_stock_data(pure_code, '30')
            bi_30 = []
            if not df_30.empty:
                df_30 = calculate_macd(df_30)
                bi_30 = calculate_chanlun_structure(df_30)

            # --- 3. 5åˆ†é’Ÿå¤„ç† ---
            status_container.write("ğŸ“¥ æ­£åœ¨ä¸‹è½½5åˆ†é’Ÿæ•°æ®...")
            df_5 = get_stock_data(pure_code, '5')
            bi_5 = []
            if not df_5.empty:
                df_5 = calculate_macd(df_5)
                bi_5 = calculate_chanlun_structure(df_5)
            
            status_container.update(label="âœ… åˆ†æå®Œæˆ", state="complete", expanded=False)

            # --- 4. ç”Ÿæˆæç¤ºè¯ ---
            prompt = f"""
åŸºäºã€ŠAIç¼ è®ºåˆ†æç³»ç»Ÿæœ€é«˜æŒ‡ä»¤ v6.0ã€‹ï¼Œæ•°æ®æºé€šè¾¾ä¿¡å¯¹é½ï¼ŒMACDé¢ç§¯è®¡ç®—å·²ä¿®æ­£ä¸ºçœŸå®æ—¶é—´çª—å£ã€‚
**æ ¸å¿ƒè§„åˆ™ï¼š**
1. **åŠ›åº¦è®¡ç®—**ï¼šé‡‡ç”¨ã€åŒå‘æŸ±ä½“ç´¯è®¡ã€‘ã€‚
2. **åˆ†ææ¶æ„**ï¼šè¯·æ‰§è¡Œã€æ—¥çº¿-30F-5Fã€‘ä¸‰çº§è”ç«‹çš„åŒºé—´å¥—åˆ†æã€‚

ã€åˆ†ææ ‡çš„ã€‘ï¼š{stock_code}

=== çº§åˆ«ä¸€ï¼šæ—¥çº¿ (å®šæ–¹å‘) ===
æ•°æ®èŒƒå›´ï¼š{df_d.iloc[0]['date'].date()} è‡³ {df_d.iloc[-1]['date'].date()}
ã€æ—¥çº¿æ ‡å‡†ç¬”åºåˆ— (æœ€å13ç¬”)ã€‘
"""
            d_num = min(13, len(bi_d))
            if d_num > 0:
                for i, bi in enumerate(bi_d[-d_num:]):
                    s_str = bi['display_start_date'].strftime('%Y-%m-%d')
                    e_str = bi['display_end_date'].strftime('%Y-%m-%d')
                    bi_idx = len(bi_d) - (d_num - 1) + i
                    area_desc = "çº¢ç§¯" if bi['direction'] == 'å‘ä¸Š' else "ç»¿ç§¯"
                    prompt += f"- ç¬”{bi_idx} [{bi['direction']}]: {s_str} -> {e_str} | ä»·:{bi['start_price']}->{bi['end_price']} | {area_desc}:{bi['macd_area']} | DIFæå€¼:{bi['peak_dif']} | å‡é‡:{bi['avg_vol']}ä¸‡\n"
            
            if bi_d:
                unf = analyze_unformed_segment(df_d, bi_d[-1])
                if unf:
                    area_type = "çº¢ç§¯" if unf['current_dir'] == 'å‘ä¸Š' else "ç»¿ç§¯"
                    prompt += f"""
ã€æ—¥çº¿å½“ä¸‹çŠ¶æ€ (æœªæˆç¬”æ®µ)ã€‘
- è¿è¡Œ: {unf['count']}å¤©
- æ–¹å‘: {unf['current_dir']} ({unf['status']})
- æå€¼: é«˜{unf['high']} / ä½{unf['low']} / æ”¶{unf['close']}
- åŠ›åº¦: MACD{area_type} {unf['macd_area']}, DIFæå€¼ {unf['peak_dif']}
"""

            prompt += "\n=== çº§åˆ«äºŒï¼š30åˆ†é’Ÿ (æ‰¾ä¹°å–ç‚¹) ===\n"
            if bi_30:
                d30_num = min(13, len(bi_30))
                prompt += f"ã€30åˆ†é’Ÿæ ‡å‡†ç¬”åºåˆ— (æœ€å{d30_num}ç¬”)ã€‘\n"
                for i, bi in enumerate(bi_30[-d30_num:]):
                    s_str = bi['display_start_date'].strftime('%m-%d %H:%M')
                    e_str = bi['display_end_date'].strftime('%m-%d %H:%M')
                    bi_idx = len(bi_30) - (d30_num - 1) + i
                    area_desc = "çº¢ç§¯" if bi['direction'] == 'å‘ä¸Š' else "ç»¿ç§¯"
                    prompt += f"- ç¬”{bi_idx} [{bi['direction']}]: {s_str} -> {e_str} | ä»·:{bi['start_price']}->{bi['end_price']} | {area_desc}:{bi['macd_area']} | DIFæå€¼:{bi['peak_dif']}\n"
                
                unf30 = analyze_unformed_segment(df_30, bi_30[-1])
                if unf30:
                    prompt += f"""
ã€30åˆ†é’Ÿå½“ä¸‹çŠ¶æ€ã€‘
- æ–¹å‘: {unf30['current_dir']}
- åŠ›åº¦: MACD{("çº¢ç§¯" if unf30['current_dir'] == 'å‘ä¸Š' else "ç»¿ç§¯")} {unf30['macd_area']}, DIFæå€¼ {unf30['peak_dif']}
"""
            
            prompt += "\n=== çº§åˆ«ä¸‰ï¼š5åˆ†é’Ÿ (ç²¾å‡†ç‹™å‡») ===\n"
            if bi_5:
                d5_num = min(20, len(bi_5))
                prompt += f"ã€5åˆ†é’Ÿæ ‡å‡†ç¬”åºåˆ— (æœ€å{d5_num}ç¬”)ã€‘\n"
                for i, bi in enumerate(bi_5[-d5_num:]):
                    s_str = bi['display_start_date'].strftime('%dæ—¥%H:%M')
                    e_str = bi['display_end_date'].strftime('%dæ—¥%H:%M')
                    bi_idx = len(bi_5) - (d5_num - 1) + i
                    area_desc = "çº¢ç§¯" if bi['direction'] == 'å‘ä¸Š' else "ç»¿ç§¯"
                    prompt += f"- ç¬”{bi_idx} [{bi['direction']}]: {s_str} -> {e_str} | ä»·:{bi['start_price']}->{bi['end_price']} | {area_desc}:{bi['macd_area']} | DIFæå€¼:{bi['peak_dif']}\n"
                
                unf5 = analyze_unformed_segment(df_5, bi_5[-1])
                if unf5:
                    prompt += f"""
ã€5åˆ†é’Ÿå½“ä¸‹çŠ¶æ€ã€‘
- æ–¹å‘: {unf5['current_dir']}
- æå€¼: é«˜{unf5['high']} / ä½{unf5['low']} / æ”¶{unf5['close']}
- åŠ›åº¦: MACD{("çº¢ç§¯" if unf5['current_dir'] == 'å‘ä¸Š' else "ç»¿ç§¯")} {unf5['macd_area']}, DIFæå€¼ {unf5['peak_dif']}
"""
            else: prompt += "ï¼ˆ5åˆ†é’Ÿæ•°æ®ä¸è¶³ï¼‰\n"

            prompt += """
ã€ä½ çš„ä»»åŠ¡ã€‘
1. **åŒºé—´å¥—å®šä½**ï¼šåˆ©ç”¨ 5åˆ†é’Ÿæ•°æ®è§£æ 30åˆ†é’Ÿæœªå®Œæˆæ®µçš„å†…éƒ¨ç»“æ„ã€‚
2. **ç‹™å‡»ç‚¹è®¡ç®—**ï¼šè®¡ç®—å‘¨ä¸€å¼€ç›˜çš„ç²¾ç¡®ä»‹å…¥ç‚¹ï¼ˆ5FäºŒä¹°/ç±»äºŒä¹°ï¼‰åŠç¡¬æ­¢æŸä½ã€‚
3. **é£æ§æŒ‡ä»¤**ï¼šç»™å‡ºæ¯«ç§’çº§æ­¢æŸæ¡ä»¶ã€‚
"""
            st.subheader("ğŸ“‹ ç”Ÿæˆçš„ AI æç¤ºè¯")
            st.code(prompt, language="markdown")
            
        except Exception: 
            status_container.update(label="âŒ å‘ç”Ÿé”™è¯¯", state="error")
            st.error(traceback.format_exc())

if __name__ == "__main__": 
    main()
